{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "858Jv9jkyBOB"
   },
   "source": [
    "# ANLP 2021 - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb4wg4-qyBOE"
   },
   "source": [
    "Yerkezhan Abdullayeva, (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lajBnnUlyBOF"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Monday, January 17, 2022, 23:59</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWW1ajryBOF"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE**\n",
    "<br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-3.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes marked by the comment \"#student solution/discussion here\" must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle. In case of questions, you can contact the TAs and the instructors via the usual channels.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA4qX077yBOG"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this assignment, you will implement an LSTM model and train it to generate text, one character at a time. (So note: We're asking you to create a character-level model; in the lectures, we've so far only seen word-level models. Think about what the difference is, and what its practical consequences are.)\n",
    "\n",
    "For training, we prepared two text files (train and test) containing passages from Charles Dickens' novels (dickens_train.txt, dickens_test.txt).\n",
    "\n",
    "You should use the PyTorch machine learning library to implement this exercise.\n",
    "\n",
    "- Instructions to install PyTorch can be found here: <http://pytorch.org/>\n",
    "- The introductory tutorial we prepared for PyTorch is attached to the assignment: pytorch_tutorial.ipynb\n",
    "- Some PyTorch examples for an in depth overview: <https://github.com/jcjohnson/pytorch-examples>\n",
    "- Another common quickstart tutorial is this [PyTorch 60 Minutes Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "(But don't get carried away: For this assignment, you mostly need the very straightforward elements from the `nn` module in PyTorch that implement the layers that you've learned about, such as RNNS, LSTMs, embeddings.)\n",
    "\n",
    "This assignment is designed to be runnable on a decent CPU. With a 2-layer LSTM and hidden size of 128, it takes ~20 minutes to train while with hidden size of 512, it takes ~2 hours. Please take this into consideration while doing this assignment. \n",
    "\n",
    "Alternatively you can also use Google Colab <https://colab.research.google.com/> by uploading your notebook there, which gives you access to a GPU. (Check that you are indeed using the GPU, via `print(torch.cuda.is_available()`.) However, please keep mind as there is limitation for the free edition (i.e. 'maximum lifetime' of 12 hours).\n",
    "\n",
    "\n",
    "The goal of this assignment is to get you to specify a simple network, and play around with its hyperparameters to explore how they affect the output. This is why we're providing you with a lot of code, to ensure that the basic housekeeping is taken care of.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_yHFFSVyBOH"
   },
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`). (What do you think is the use of this step?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YspSyOk7yIpR",
    "outputId": "d64b3b63-89d8-49fa-d36f-a773ce3dfbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 5.1 MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.2\n"
     ]
    }
   ],
   "source": [
    "! pip install unidecode\n",
    "#I dont know why but in google collab I always have to redownload unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WpgbNyByBOI",
    "outputId": "e4652f03-4b1f-48e3-aea8-42ff07325199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1136673\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('dickens_train.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knjrN9_2yBOK"
   },
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGyp9APbyBOK",
    "outputId": "7292eef7-b644-4d23-a0e5-b94b83b7bbba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y precious!\" and \"my bird!\" and spreading\n",
      "her golden hair aside over her shoulders with great pride and care.\n",
      "\n",
      "  \"And you in brown!\" she said, indignantly turning to Mr. Lorry;\n",
      "\"couldn't you tell her w\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len -1) \n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uVJzhxvyBOL"
   },
   "source": [
    "# Build the Model (30 points)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The model that you are asked to build will take as input characters up to step $t-1$ and is expected to produce a distribution over characters at step $t$ (which can then be used to sample one character from that distribution). \n",
    "\n",
    "There are three layers: one layer that maps the input character into its embedding, one LSTM layer (which may itself have multiple layers) that operates on that embedding and a hidden and cell state, and a decoder layer that outputs the probability distribution.\n",
    "\n",
    "The beauty of frameworks such as PyTorch is that you can express this pretty directly in code, adding (pre-defined) layers to your network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gSjw3fhjyBOL"
   },
   "outputs": [],
   "source": [
    "#student solution/discussion here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#you can import additional libraries if needed.\n",
    "\n",
    "# Here is a pseudocode to help with your LSTM implementation. \n",
    "# You can add new methods and/or change the signature (i.e., the input parameters) of the methods.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, dimension_input, dimention_hidden, output_size, layers_size,emb_dim = 100):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.dimension_input = dimension_input\n",
    "        self.layers_size = layers_size\n",
    "        self.dimention_hidden = dimention_hidden\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = 1\n",
    "        self.EMBD = nn.Embedding(dimension_input, dimention_hidden)\n",
    "        #100 is the size of each embedding vector\n",
    "        self.lstm = nn.LSTM(dimention_hidden, dimention_hidden, layers_size, batch_first = True)\n",
    "         #This is our recurrence layer-it is our decoder and it grasps  the layer of embedded charcater numbers like number of charachters(inputs) and outputs of hidden state\n",
    "        #from the training part:decoder = LSTM(n_characters, dimention_hidden, n_characters, n_layers).\n",
    "        self.linear_layer = nn.Linear(dimention_hidden, output_size)\n",
    "        #This is linear layer. \n",
    "\n",
    "    \n",
    "    def forward(self, x,hidden_lay):\n",
    "        embedding = self.EMBD(x) \n",
    "        result1, (self.hidden_zeros, self.cell_zeros) = self.lstm(embedding.view(1, 1, -1), hidden_lay) \n",
    "        result2 = self.linear_layer(result1.view(1, -1)) \n",
    "        #here we will get output of the linear layer\n",
    "        return result2, (self.hidden_zeros, self.cell_zeros)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):   \n",
    "        #we have already assigned 1 to batch size above\n",
    "        #creating tensors\n",
    "        \n",
    "        self.hidden_zeros = torch.zeros((self.layers_size, self.batch_size, self.dimention_hidden))\n",
    "        self.cell_zeros = torch.zeros((self.layers_size, self.batch_size, self.dimention_hidden)) \n",
    "        return self.hidden_zeros, self.cell_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpEBJR9cyBOM"
   },
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzjweMgUyBON"
   },
   "source": [
    "Each chunk of the training data needs to be turned into a sequence of numbers (of the lookups), specifically a `LongTensor` (used for integer values). This is done by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqelzYnuyBOO",
    "outputId": "903dbf5a-3f35-4265-e1b4-026eb0eb6b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable \n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsMkzXdVyBOO"
   },
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The inputs will be all characters *up to the last*, and the targets will be all characters *from the first*. So if our chunk is \"abc\" the inputs will correspond to \"ab\" while the targets are \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VxXW6DNryBOP"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hH8AWVqyBOP"
   },
   "source": [
    "Play around with these functions to understand what they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i80obY7IyBOP"
   },
   "source": [
    "# Generating\n",
    "\n",
    "We also provide a generator function that shows you how you can sample from your model (and how we expect the interface to work). \n",
    "\n",
    "`decoder` is your model that is passed into the function. To start generating, we pass a priming string to start building up the hidden state, from which we then generate one character at a time. To generate strings with the network, we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cFtP9v9WyBOQ"
   },
   "outputs": [],
   "source": [
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden, cell = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, (hidden, cell) = decoder(prime_input[p], (hidden, cell)) \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, (hidden, cell) = decoder(inp, (hidden, cell))\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-NqKtJXyBOQ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oFFw_n-yBOR"
   },
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dijwLdKdyBOR"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaXMi1hEyBOR"
   },
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hwpj3kPxyBOS"
   },
   "outputs": [],
   "source": [
    "def train(decoder, decoder_optimizer, inp, target):\n",
    "    hidden, cell = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, (hidden, cell) = decoder(inp[c], (hidden, cell))\n",
    "        loss += criterion(output, target[c].view(1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() /chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWKirP8jyBOS"
   },
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aTztvjHyBOS",
    "outputId": "367e673c-7708-4853-fa29-cd6f9aec9a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 21s (100 3.3333333333333335%) 2.4668]\n",
      "ATo, to rarstiof of ad wosder of I \n",
      "ak an gile, \n",
      "obit nhoede be the ot\n",
      "tbar aded\n",
      "thubk pouved the on  \n",
      "\n",
      "[0m 44s (200 6.666666666666667%) 2.3689]\n",
      "At ofut iblot \n",
      "lerkend it whoon to her\n",
      "dessad banai\n",
      "mey the hik to some the nere med  sean caveellit  \n",
      "\n",
      "[1m 7s (300 10.0%) 2.4660]\n",
      "A' what of fad missill.' I I the shich link the ice whivee said so wacand and bever bace or disted if \n",
      "\n",
      "[1m 30s (400 13.333333333333334%) 2.3187]\n",
      "Ad wasped and thir. I \n",
      "bewh hissed she in and unted the Hart a he mewtrands me it ampearker here\n",
      "of s \n",
      "\n",
      "[1m 52s (500 16.666666666666664%) 2.0185]\n",
      "A? Clould the bus bemally seney.  You pritid. Mr, and four shere ondear the pain, boncion apter Hous  \n",
      "\n",
      "[2m 15s (600 20.0%) 2.0930]\n",
      "A Mr. reyen and the rooks and abed it\n",
      "ands a pate and. Mhe \n",
      "Rove and that Mr. younted berer.\n",
      "\n",
      "  That  \n",
      "\n",
      "[2m 37s (700 23.333333333333332%) 2.0602]\n",
      "Aver fill calk I been whor willing to of to nower and bone to mong all qut he to had and deely the he \n",
      "\n",
      "[2m 59s (800 26.666666666666668%) 2.0407]\n",
      "Aswers straished uf exprown and come his meat to very to for stack sulishet yout was cockle I the whe \n",
      "\n",
      "[3m 22s (900 30.0%) 1.7974]\n",
      "A't whin untart that hay hought her a had of hearring sindening tirflether.\n",
      "\n",
      "  \"Whew sury her a becam \n",
      "\n",
      "[3m 44s (1000 33.33333333333333%) 1.7741]\n",
      "Ad moessed it my lave wo look the ranecaur.  Is with some witther in his the and you a me was not\n",
      "ver \n",
      "\n",
      "[4m 6s (1100 36.666666666666664%) 1.6749]\n",
      "A' puspiding ase the shimp andid the prettess I don the aghing lave and or such of the you hem mestly \n",
      "\n",
      "[4m 29s (1200 40.0%) 1.7732]\n",
      "At turnissey that he good!d the in of Bud, to mores fromed herse, as stoud her hound tone for Stung u \n",
      "\n",
      "[4m 51s (1300 43.333333333333336%) 1.6211]\n",
      "As one.  'Codmarer, said some which to preation, anchance-\n",
      "the it agout, if he\n",
      "were might otherged om \n",
      "\n",
      "[5m 13s (1400 46.666666666666664%) 1.9166]\n",
      "At thom the for I whosked to sid.  Ount by who could at atted briadiars do antaage, the chariturnly w \n",
      "\n",
      "[5m 35s (1500 50.0%) 1.8024]\n",
      "A macly.  Tread, them prenfil Creat,'\n",
      "\n",
      "''Do have and some, and\n",
      "Mr. Gecess, and have pleance him, old  \n",
      "\n",
      "[5m 58s (1600 53.333333333333336%) 1.6184]\n",
      "A'\n",
      "\n",
      "'When his near Mr. Crigething at me his on \n",
      "to deared fide not them becert.\"\n",
      "\n",
      "'I whot enticuls.   \n",
      "\n",
      "[6m 21s (1700 56.666666666666664%) 1.6090]\n",
      "And eige my said.  A'ming now and foller, and and such from orring and sinvere your overes a paspect  \n",
      "\n",
      "[6m 43s (1800 60.0%) 1.6518]\n",
      "Allt hgable or atchion he was of a all the wass the caitted show her at on that litty, he hall, not s \n",
      "\n",
      "[7m 5s (1900 63.33333333333333%) 1.9527]\n",
      "Al The - I meminnight own take to a pesies pull to was and the aswell at sentenide to is a prience.\"\n",
      " \n",
      "\n",
      "[7m 27s (2000 66.66666666666666%) 1.9217]\n",
      "And \n",
      "indeade than the walk of you am choped.\n",
      "\n",
      "Mindince that and should this sace he presseage of mome \n",
      "\n",
      "[7m 49s (2100 70.0%) 1.9283]\n",
      "Agorderord at the rides side.\n",
      "\n",
      "                                          Clight an gleled-remare to b \n",
      "\n",
      "[8m 11s (2200 73.33333333333333%) 2.1630]\n",
      "An so repoes\n",
      "so so coone adver it like with before his beg the mull to an occollecter to way of begge \n",
      "\n",
      "[8m 33s (2300 76.66666666666667%) 1.6111]\n",
      "As here and them all the citisficife, and here!  Creturt of the man the would.  Jerringing it an arm  \n",
      "\n",
      "[8m 56s (2400 80.0%) 1.8792]\n",
      "A\n",
      "the will dowbout dess, 'she said, and would of on the tooked thiss, while out is sloom at my\n",
      "deeps  \n",
      "\n",
      "[9m 18s (2500 83.33333333333334%) 1.8813]\n",
      "Ace fold pan a said as pasfouring in procaunt on oor.'\n",
      "\n",
      "'Peggotty saws left nothed at but ofter of th \n",
      "\n",
      "[9m 41s (2600 86.66666666666667%) 1.7805]\n",
      "An the arm in the know, bed in the parted by you hop, the Eday incomsten while.'\n",
      "\n",
      "He had me sides coa \n",
      "\n",
      "[10m 3s (2700 90.0%) 1.7147]\n",
      "AT I have all appeal at he say depoves, to it maving his the for his key winder, how a pretentant Peg \n",
      "\n",
      "[10m 25s (2800 93.33333333333333%) 1.5804]\n",
      "A no \n",
      "quite is hopeticulation and look all being-to a cart I was no room to the finary, 'I voirertent \n",
      "\n",
      "[10m 48s (2900 96.66666666666667%) 1.6247]\n",
      "AR WOT DEEL Meggotty?' said\n",
      "Mr. Jasped and had charlly was a you burdles, sat Chould beir the asterin \n",
      "\n",
      "[11m 10s (3000 100.0%) 1.3550]\n",
      "Ay a was it was a fire, I weest house of that I cause.  He construct of the farkle, and I was up by c \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 128\n",
    "n_layers = 2\n",
    "\n",
    "lr = 0.005\n",
    "decoder = LSTM(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    loss = train(decoder, decoder_optimizer, *random_training_set())\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "        print(generate(decoder, 'A', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg/ plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaV-Kqi0yBOT"
   },
   "source": [
    "\n",
    "# ***1.What do these parameters do?***\n",
    "\n",
    "**n_epochs = 3000: ***\n",
    "This means how many  times that the algorithm  goes through the dickens_training_data and how many times it will learn.\n",
    "**print_every = 100:** when the epoch reaches 100 it prints hundreds one by one.For example,200,300.....\n",
    "\n",
    "**hidden_size = 128:**  How many features we have in the hidden layer\n",
    "\n",
    "**n_layers = 2:**: the amount of hidden layers in  the LSTM.\n",
    "\n",
    "**lr = 0.005:**: How big out steps of the learning rate in gradient descent.We use it to find the minumun loss in gradient descent.\n",
    "\n",
    "\n",
    "# **What is happening inside the training loop?** \n",
    "In the loop we have the:\n",
    "\n",
    "**time**-how long it took to learn for the model,the algorith for it has been written for us,\n",
    "\n",
    "**every 100th epoch's number**-as I have stated above,the model is printing with a difference of 100 epochs.\n",
    "\n",
    "**Percentage**-how many percent is already trained.\n",
    "\n",
    "**Loss**- the result of the loss function\n",
    "\n",
    "**decoder = LSTM(n_characters, hidden_size, n_characters, n_layers)**-every epoch is being trained by this generation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1ln4XaQyBOT"
   },
   "source": [
    "Explain in your words what is going on here. What do these parameters do, what is happening inside the training loop? (**bonus question**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGpnq5w2yBOT"
   },
   "source": [
    "# Hyperparameter tuning (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3FDRs9DyBOT"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Building neural networks is to some extent more an art than a science. As we have seen above, there are several hyperparameters (i.e., parameters that are not optimized during learning, but that determine the shape of the network), and their setting influences the performance. In this problem, you're asked to *tune* these hyperparameters (that is, optimize heuristically, rather than using for example stochastic gradient descent). You can try to do this systematically (how?), or just in general explore what changing the parameter does to the performance. (Keep in mind the time it takes to train again for each setting.)\n",
    "\n",
    "To do so, you need a target. We'll use bits per character (BPC) over the entire the test set `dickens_test.txt`. \n",
    "BPC is defined as the empirical estimate of the cross-entropy between the target distribution and the model output in base 2. \n",
    "\n",
    "(Hint1: You can adapt the formula for word-level cross-entropy given in your text book (chapter 9) to character-level as $-\\frac{1}{T}*\\sum_{i=1}^{T}log{_2}{m(x_t)}$ where T is the length of input string and $x_t$ is the true character in input string at location $t$.)\n",
    "\n",
    "(Hint2: Tune one parameter at a time) \n",
    "\n",
    "(Hint3: Keep a log of your experiments for \"parameters used --> minimum loss value\")\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1FdAmA0OyBOU"
   },
   "outputs": [],
   "source": [
    "#To check which hyperparameter are the most suitable lets tranform our training into s function.\n",
    "\n",
    "def parameters_tuning(hidden_size, n_layers, lr):\n",
    "\n",
    "    n_epochs = 3000 \n",
    "    print_every = 100 \n",
    "    plot_every = 10\n",
    "    #decoder is your model that is passed into the function.\n",
    "    decoder = LSTM(n_characters, hidden_size, n_characters, n_layers)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    loss_avg = 0\n",
    "#we just add some returning values,so we can call it later\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss = train(decoder, decoder_optimizer, * random_training_set())\n",
    "        loss_avg += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "            print(generate(decoder, 'A', 100), '\\n')\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(loss_avg/ plot_every)\n",
    "            loss_avg = 0\n",
    "    \n",
    "    return decoder, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI9eatxZyBOU",
    "outputId": "018072f2-9ea8-4dd8-fd6a-042d59bbc42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of test file is 5476\n"
     ]
    }
   ],
   "source": [
    "file_test = unidecode.unidecode(open('dickens_test.txt').read()) \n",
    "test_tensor = char_tensor(file_test)\n",
    "#Let's see the length of our test data\n",
    "print('The length of test file is',len(file_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-SLaoag6yBOV"
   },
   "outputs": [],
   "source": [
    "def bits_per_character(decoder, n_characters, goal): \n",
    "    \n",
    "    hidden, cell = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    b_per_character = 0\n",
    "\n",
    "    for c in range(len(n_characters)):\n",
    "        output, (hidden, cell) = decoder(n_characters[c], (hidden, cell))\n",
    "        softmax = nn.Softmax()\n",
    "        out = softmax(output)\n",
    "    \n",
    "        out = out.detach().numpy()\n",
    "        out = out.squeeze()\n",
    "        t = goal[c].view(1).numpy() \n",
    "        b_per_character += math.log(out[t[0]],2) \n",
    "\n",
    "    return - (b_per_character / len(n_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKyxvsnpyBOY",
    "outputId": "45d78c2b-af23-4404-f4fa-a2fd0ddd2d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model with hidden size 64, number of layers 2, learning rate 0.005\n",
      "[0m 17s (100 3.3333333333333335%) 3.3040]\n",
      "ATteg t.  yd  intg ar tennts, oles hdjim od  \n",
      "hee 'siiot\n",
      "b beom ocetet ta \n",
      "an an tmalie  hwel lif ols \n",
      "\n",
      "[0m 34s (200 6.666666666666667%) 2.7746]\n",
      "Au\n",
      "binl' tof an I M.  om kere nerin, po\n",
      "ked cwis eond lorive sorley mee.  potad she\n",
      "sar rerhese cangt \n",
      "\n",
      "[0m 52s (300 10.0%) 2.4937]\n",
      "Ashos wreres he the and  at fargtot indeu serous hored om the ar  u wingsed ind \n",
      "erey\n",
      "iny be\n",
      "tike ko  \n",
      "\n",
      "[1m 9s (400 13.333333333333334%) 2.4094]\n",
      "A0f  yows. 'oe sasterser, belmere yours warg? \n",
      "\n",
      "fut opeseel yoope ongt ous whanged Pe Mler slle cage  \n",
      "\n",
      "[1m 27s (500 16.666666666666664%) 2.1796]\n",
      "As in to bath in the ut lech the mefuine ai fa. shame Xaiky a med fathy in and aning iny as if he a i \n",
      "\n",
      "[1m 45s (600 20.0%) 2.0872]\n",
      "Af will and anch am me.\n",
      "\n",
      "and cat ou verod she alt\n",
      "his the the suss.\n",
      "\n",
      "'fo cto were me, and\n",
      "the, fouste \n",
      "\n",
      "[2m 2s (700 23.333333333333332%) 1.9187]\n",
      "A\n",
      "\n",
      " more I so wiing my and sinttreld hal\n",
      "the.  I wardy\n",
      "and he, the finishe, pone the ceed' she dor th \n",
      "\n",
      "[2m 19s (800 26.666666666666668%) 2.0006]\n",
      "And died at thas thell benharganher that, b, busing and in the litt\n",
      "wang be cord land one hy reagcale \n",
      "\n",
      "[2m 37s (900 30.0%) 2.0492]\n",
      "A'd to coll ever hiten all to\n",
      "lnost eated to sead to \n",
      "in on the nom tase at was his one Il coned the  \n",
      "\n",
      "[2m 55s (1000 33.33333333333333%) 2.0817]\n",
      "At surdenten were for butnion thas tound turniteas, of foly hat with we nout the the sal sulurd to th \n",
      "\n",
      "[3m 12s (1100 36.666666666666664%) 2.1745]\n",
      "And cind in thow for her it a Wean, a speaged that resing alliof asPentnlassersy hand\n",
      "of as bucall to \n",
      "\n",
      "[3m 29s (1200 40.0%) 2.2021]\n",
      "A roon and granted sanster's have his ciust he nother of bist in want it And stise the best my, to hi \n",
      "\n",
      "[3m 47s (1300 43.333333333333336%) 1.9111]\n",
      "A I Lonsed thanded ong  I compeced the rould as tom I doal upones, \n",
      "there of \n",
      "o his in Cage bundelf w \n",
      "\n",
      "[4m 5s (1400 46.666666666666664%) 2.1749]\n",
      "A  'to hatter.  Notter, wher thing stome is pridper sam said the mornter me a\n",
      "lood the falken to foun \n",
      "\n",
      "[4m 22s (1500 50.0%) 1.9479]\n",
      "A seing with limed sull, of om bus in ther of to anfics of hank was ho caunds crothlemed sil Froce sm \n",
      "\n",
      "[4m 40s (1600 53.333333333333336%) 1.8929]\n",
      "An Serefore or so thours as lally so at ry with ally in of to the her, enott bone dittest.\n",
      "\n",
      "My coarti \n",
      "\n",
      "[4m 57s (1700 56.666666666666664%) 1.9965]\n",
      "A\n",
      "   And the goout weal noverent of the fifkinly,\n",
      "and my firly out on the I Mr. a said and Mr. Lettor \n",
      "\n",
      "[5m 14s (1800 60.0%) 1.9512]\n",
      "A \"I sine?  \"The a krons a don her mine coke after, when chall lound but wither am so had made of but \n",
      "\n",
      "[5m 32s (1900 63.33333333333333%) 2.0858]\n",
      "A dreall it they the we who when so the pand a willy. \n",
      "\n",
      "  I gurting and had to the no pairs, and and  \n",
      "\n",
      "[5m 50s (2000 66.66666666666666%) 1.8456]\n",
      "A ou light and a gacking of in that \n",
      "for that which manden!  Noas I froume bertout.\n",
      "\n",
      "  I\n",
      "candle diwtl \n",
      "\n",
      "[6m 7s (2100 70.0%) 2.0286]\n",
      "A befues herout, the were the sountit, 'if the bear whor it this dall, ay\n",
      "so the roon not of hiss ing \n",
      "\n",
      "[6m 24s (2200 73.33333333333333%) 2.1690]\n",
      "A are I cather of mastore the pury was, and Peggrould in to be tright the boubled of the dich with un \n",
      "\n",
      "[6m 41s (2300 76.66666666666667%) 2.0701]\n",
      "A Mr. The wiuting \n",
      "a drow?  I his dopainion and thear?  I the was sort the saase a said a sthe shind  \n",
      "\n",
      "[6m 58s (2400 80.0%) 2.0534]\n",
      "A with gave terter \n",
      "and at the roown his\n",
      "dishian his the sapself brirned it lead the Piving inder, or \n",
      "\n",
      "[7m 14s (2500 83.33333333333334%) 1.7406]\n",
      "A stay,' the wonding of the done lone of to the\n",
      "remeding susitale, it whal has confulled\n",
      "that one tha \n",
      "\n",
      "[7m 31s (2600 86.66666666666667%) 1.8129]\n",
      "A had all her look.'\n",
      "\n",
      "'Never firded amourdotere the claster the rvight the compaid, a the known he ha \n",
      "\n",
      "[7m 47s (2700 90.0%) 2.3390]\n",
      "A  I and \n",
      "suskle come there was theve the cheling the saries a mace and a much the came goon,\n",
      "I the \n",
      " \n",
      "\n",
      "[8m 4s (2800 93.33333333333333%) 1.7804]\n",
      "A had be as he was the had at you reech I she go, and he now ancard of anlest good a sore of the fare \n",
      "\n",
      "[8m 23s (2900 96.66666666666667%) 1.9336]\n",
      "A  \n",
      "For take have Ho such fin and own had, \n",
      "Dother manded pevered and that you and of his has afto- a \n",
      "\n",
      "[8m 40s (3000 100.0%) 1.8762]\n",
      "A\n",
      "\n",
      "'I fentwop, but the was leend am this bread of thewhichs, then est, and amhorther, was betunk lict \n",
      "\n",
      "Minimal loss  1.7532012786865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPC value  2.68350434152888\n",
      "Learning model with hidden size 128, number of layers 2, learning rate 0.005\n",
      "[0m 21s (100 3.3333333333333335%) 2.3759]\n",
      "A,\"\n",
      " zals I me\n",
      "artithimtel.'\n",
      "\n",
      "\n",
      "kend an Nins; cice oning ing an. sot cof non: with and h. os yon lon c \n",
      "\n",
      "[0m 43s (200 6.666666666666667%) 2.1711]\n",
      "APl ith agy \n",
      "lave im shos it Pes shid she \n",
      "the she this he sith me nomed his wat, of sto had wherst a \n",
      "\n",
      "[1m 6s (300 10.0%) 2.0851]\n",
      "APrd and reaps one it in his\n",
      "yout hit\n",
      "the ind bost shis to Domgs the fare, had by sold, ant and hims  \n",
      "\n",
      "[1m 27s (400 13.333333333333334%) 2.1142]\n",
      "AH ly her that musfer her busost hay her lith had\n",
      "send was ste the nower, to and a a dew had was aftr \n",
      "\n",
      "[1m 50s (500 16.666666666666664%) 1.8146]\n",
      "A\"he streem by you, were.  It MT\n",
      "CJEIghister to shoung.'\n",
      "\n",
      "\n",
      "  He weac bothing sapping Rong he have ban \n",
      "\n",
      "[2m 12s (600 20.0%) 2.2503]\n",
      "A7NE I Pevible notor deamang Jilling, Mr. Colry whoon to cou of timefcly gotseditife strult who hat t \n",
      "\n",
      "[2m 33s (700 23.333333333333332%) 2.1443]\n",
      "As sair he wond I the as the retaid his dery dour then an it his curdstosady, whan his repted our.\n",
      "\n",
      "' \n",
      "\n",
      "[2m 55s (800 26.666666666666668%) 1.9451]\n",
      "Apart and in the fom his in his hird her remise thing to reage not not me streare the\n",
      "cally, they who \n",
      "\n",
      "[3m 17s (900 30.0%) 1.7965]\n",
      "Aw stothe had a have my preveranges:  I was and the had but for a pam macty dencallure his me intesct \n",
      "\n",
      "[3m 39s (1000 33.33333333333333%) 1.7747]\n",
      "Apse worstered give the\n",
      "wards who was leef arjt at shis time of thy provell dick; do your the gencame \n",
      "\n",
      "[4m 2s (1100 36.666666666666664%) 1.7932]\n",
      "Arrious but it, the off in all faelunge \n",
      "you wat dail it Deelf, sore at it dible to one the knowife b \n",
      "\n",
      "[4m 23s (1200 40.0%) 1.7493]\n",
      "ARw:\n",
      "\n",
      "'Howing of the express, redcallious.  The wistings of to and reposening speinise, and there gro \n",
      "\n",
      "[4m 45s (1300 43.333333333333336%) 1.9668]\n",
      "An the jort his sight real!' he distion his me to the me.  I too the see gair snomessed elper; the fi \n",
      "\n",
      "[5m 8s (1400 46.666666666666664%) 1.9034]\n",
      "And it the had put old may for your sung into hold in the any was wyel conseriles the shopsiles that  \n",
      "\n",
      "[5m 30s (1500 50.0%) 1.6608]\n",
      "Aln the fived to ve\n",
      "cours this me.  Never, and suid, the measher to\n",
      "chorised only with the had dear a \n",
      "\n",
      "[5m 52s (1600 53.333333333333336%) 1.8599]\n",
      "Aharant the growent am of luce, - Lorming up in his the dreed, but in the frirstone, that \n",
      "thelred ca \n",
      "\n",
      "[6m 14s (1700 56.666666666666664%) 1.8384]\n",
      "Ath that he ovred himse and shresed the presations at her of sirn the chellage a ladd inother, as tha \n",
      "\n",
      "[6m 37s (1800 60.0%) 1.9678]\n",
      "Aght into I day the Landy sile consiled that I an!' said' sile him evenare him of chen my ear the lif \n",
      "\n",
      "[6m 59s (1900 63.33333333333333%) 1.9255]\n",
      "Ahry sedy, it went yethes, and brooring supsion, ald you dearing acly the it a distions prosesness is \n",
      "\n",
      "[7m 21s (2000 66.66666666666666%) 1.9243]\n",
      "ALnd out of that to! says \n",
      "time ovet it, and going good, as if an she other more rumpured ide\n",
      "choir h \n",
      "\n",
      "[7m 42s (2100 70.0%) 1.8011]\n",
      "And of the then I on the shough motter souble back the\n",
      "way was he mother.\n",
      "\n",
      "The don't partantions woul \n",
      "\n",
      "[8m 4s (2200 73.33333333333333%) 1.5982]\n",
      "ATI How  Butted the took was rellant its seven hoperate, and I would Rather to stood which the don'ts \n",
      "\n",
      "[8m 26s (2300 76.66666666666667%) 1.7349]\n",
      "As any which and blue agreat from\n",
      "was manunt to Mr. Mird.  And I sad could way oven the teen a pittit \n",
      "\n",
      "[8m 48s (2400 80.0%) 1.7944]\n",
      "Attle dead.  The must to her sucarchly, been\n",
      "should be raught, so hundle imstrable a come of much clo \n",
      "\n",
      "[9m 11s (2500 83.33333333333334%) 1.7663]\n",
      "A, the which \n",
      "the bleet hor from the considered the stote was jushion his long hims applield world to \n",
      "\n",
      "[9m 33s (2600 86.66666666666667%) 1.6127]\n",
      "Aw Evank his bright to\n",
      "the carching fartully meanh it and been you'lr\n",
      "streated\n",
      "of the gaid to a carri \n",
      "\n",
      "[9m 55s (2700 90.0%) 1.8420]\n",
      "At! I it, and they dave is that a carreated froningly of it were soneh, and a will linpe, entlear, as \n",
      "\n",
      "[10m 17s (2800 93.33333333333333%) 1.5057]\n",
      "As a\n",
      "cherk of any sir were every one and had had get in the - and he only be old and any rought and f \n",
      "\n",
      "[10m 39s (2900 96.66666666666667%) 1.5112]\n",
      "Ancund.  The not some that a sire one I least.  He face.  H!  Somes it worn sit the colded her hands  \n",
      "\n",
      "[11m 1s (3000 100.0%) 1.9314]\n",
      "AN1G Davy. My new dealing hing a mother.\n",
      "\n",
      "'And it it, and emerpmenishop her's and it\n",
      "of it a  come pu \n",
      "\n",
      "Minimal loss  1.5792749481201172\n",
      "BPC value  2.4553991078720423\n",
      "Learning model with hidden size 256, number of layers 2, learning rate 0.005\n",
      "[0m 57s (100 3.3333333333333335%) 2.1820]\n",
      "Aket'\n",
      "\n",
      " be the a lit lat the and gild the shit of the\n",
      "the pfould wincis wy. in sace to to reokid sung \n",
      "\n",
      "[1m 53s (200 6.666666666666667%) 2.0454]\n",
      "A jowes lare to whin.  sectelden it hout mpestle\n",
      "Stiou a ald in, inting. withe stouv tha the aklagtly \n",
      "\n",
      "[2m 50s (300 10.0%) 2.1704]\n",
      "A\n",
      "  Thim the was tast up fron of bot a bace appect to taw thing contentonhep with to comtone borst, a \n",
      "\n",
      "[3m 47s (400 13.333333333333334%) 1.8603]\n",
      "A  I a but bind domen now a come desttarn of the and upon his tha varn exed to to go of tho and to st \n",
      "\n",
      "[4m 44s (500 16.666666666666664%) 2.0242]\n",
      "AMs was and ading the for onbe, and Mr. pary ablaid not, and after.\n",
      "\n",
      "'And to sive courent in ther on  \n",
      "\n",
      "[5m 40s (600 20.0%) 2.1162]\n",
      "At\n",
      "a suppent deat of caits.  It his my him with.  'As a sut the person got.\n",
      "  The powed I dehat a per \n",
      "\n",
      "[6m 37s (700 23.333333333333332%) 1.8523]\n",
      "A Qirm, and see in ip to say my done.'\n",
      "\n",
      "'Your, the\n",
      "pirod a dain-took brown s and had Mr. ''O, and Poo \n",
      "\n",
      "[7m 34s (800 26.666666666666668%) 1.6914]\n",
      "Asos in clot, said stind broft. If it intets \n",
      "of thes see to the plood abot of the the surtcomrong of \n",
      "\n",
      "[8m 31s (900 30.0%) 1.7519]\n",
      "Aut.  When ver his his priad acmorry the had me, I hard the came to it, ant. Mr. with a frupch the cr \n",
      "\n",
      "[9m 28s (1000 33.33333333333333%) 1.6710]\n",
      "As did sicilight the reice the the pounce-ltor that  \n",
      "wrising was anseltitine moself of his and mothe \n",
      "\n",
      "[10m 25s (1100 36.666666666666664%) 1.6749]\n",
      "As to out, and your her anyther of the restrommmer of tite her sucelf me wittle hand there in the Cac \n",
      "\n",
      "[11m 21s (1200 40.0%) 1.6876]\n",
      "And I had me,' doling him.  \"Do are acdectence indent's, and \n",
      "perfaces, as hearsed by rustion!' Secra \n",
      "\n",
      "[12m 18s (1300 43.333333333333336%) 1.9497]\n",
      "AT the some with a leakis, you any looker of you,' ressect ignot prath at my bent roppen it it whelen \n",
      "\n",
      "[13m 15s (1400 46.666666666666664%) 1.6441]\n",
      "And I wastor courned at I takful what the purse abject bown.  Qrische of turned the plewnous mork and \n",
      "\n",
      "[14m 13s (1500 50.0%) 1.7847]\n",
      "Afts read of reoure, with be with the someless make, with was plassed to behousiness.  Becaunt a guri \n",
      "\n",
      "[15m 10s (1600 53.333333333333336%) 1.8068]\n",
      "AR don'ths Mare, showing in sat aghed by wheverery stoffsomect, chertom to the\n",
      "fare your is the inten \n",
      "\n",
      "[16m 7s (1700 56.666666666666664%) 1.7786]\n",
      "And to account for seemed, the steption, callicants time.  He instemparan,' said that where going of  \n",
      "\n",
      "[17m 4s (1800 60.0%) 1.7030]\n",
      "Are say me had song before by the done, that I fortent from a dess, when steast man fell he find.\n",
      "\n",
      "It \n",
      "\n",
      "[18m 1s (1900 63.33333333333333%) 1.9381]\n",
      "A much ourser by confidents scan, \n",
      "to my ware remen uprake to for had be wonse upon of his fits to he \n",
      "\n",
      "[19m 0s (2000 66.66666666666666%) 1.4696]\n",
      "Agly support tood, which into a had presencts wine for expocking for the look, for it morning the had \n",
      "\n",
      "[20m 0s (2100 70.0%) 1.9168]\n",
      "Ament.  Then that I shawe, \n",
      "it.'  Hair and they day in this happen.\n",
      "\n",
      "She work a sept of excair and fo \n",
      "\n",
      "[21m 0s (2200 73.33333333333333%) 1.7701]\n",
      "ASt any hight agains a rejoint, and froes friention would wine indepied all are see\n",
      "'Freleal pleepper \n",
      "\n",
      "[21m 58s (2300 76.66666666666667%) 1.7287]\n",
      "ANother which I serted but drone.  And again oped a many any wriscorsural her\n",
      "infection him it.\n",
      "\n",
      "'As  \n",
      "\n",
      "[22m 56s (2400 80.0%) 1.6328]\n",
      "And his to the say, and you - to you, and I sat do at a getched to me, Peggotty, reput the momen in,  \n",
      "\n",
      "[23m 55s (2500 83.33333333333334%) 1.6178]\n",
      "Ang iman unlight.'\n",
      "\n",
      "'It of the Tumber, and not, and Barding all \n",
      "intorn's, but I sing, as the lake.   \n",
      "\n",
      "[24m 54s (2600 86.66666666666667%) 1.5370]\n",
      "AIs that he hast forress, and very come sat seemed to be and terrowess.  It though he the doarse obse \n",
      "\n",
      "[25m 51s (2700 90.0%) 1.3798]\n",
      "Agards, why aut, her, what my by the start, when he have roll to the carrying ward upon his great the \n",
      "\n",
      "[26m 49s (2800 93.33333333333333%) 1.9553]\n",
      "And mare by him.  Dean on that he wrapperfable and knew stare,\n",
      "commantanable up he purpain in ho gave \n",
      "\n",
      "[27m 47s (2900 96.66666666666667%) 1.7488]\n",
      "All.\"\n",
      "\n",
      "  \"I hare the wolder, at her degizably paller turnign and boot Sure delight of the bound\n",
      "that  \n",
      "\n",
      "[28m 45s (3000 100.0%) 1.4986]\n",
      "As of \n",
      "onsish in could shoulder as I partednt, and day some such\n",
      "had class that I father appossionate \n",
      "\n",
      "Minimal loss  1.5037402191162108\n",
      "BPC value  2.3807940837033374\n",
      "Learning model with hidden size 512, number of layers 2, learning rate 0.005\n",
      "[3m 24s (100 3.3333333333333335%) 2.2876]\n",
      "Ad, uer arl mere fourger. daung heind \n",
      "I ee s thoe the soes wiranger as sures I thinged, ther cef hor \n",
      "\n",
      "[6m 51s (200 6.666666666666667%) 2.3170]\n",
      "K! bears fith and sos hat to awh out as a tate ava.\n",
      "\n",
      "Wout of the no of of to beald the shie bime pe \n",
      "\n",
      "[10m 10s (300 10.0%) 2.2137]\n",
      "A(\t+?g but forpetterse yas Put and an kere ther a frethe I prises, Mrpe ed beain that rlosout it said \n",
      "\n",
      "[13m 28s (400 13.333333333333334%) 2.0355]\n",
      "AZoed onto the and be king layed Lallys, pounting to twell egged he it to and whou'\n",
      "\n",
      "The austing besh \n",
      "\n",
      "[16m 43s (500 16.666666666666664%) 2.0272]\n",
      "And and and over but ont\n",
      "be cuny his sut portthed in of appacterrent and met-he shoo Bable goot the l \n",
      "\n",
      "[19m 57s (600 20.0%) 1.8364]\n",
      "An of her ther was strome up able\n",
      "be he was man wany mark, a care farring old whe\n",
      "come intianed\n",
      "of to \n",
      "\n",
      "[23m 11s (700 23.333333333333332%) 2.0558]\n",
      "AH of be with her this palliours, with is he a quads was of being\n",
      "sipled son lect by wonger, the rost \n",
      "\n",
      "[26m 30s (800 26.666666666666668%) 1.8748]\n",
      "And as is the goar that that the listair Betch such in you facking uppares, and Mr.  You have has pur \n",
      "\n",
      "[29m 51s (900 30.0%) 1.6516]\n",
      "A whother her un when the much sirned remprosparkle the the self, cal gefanaint my she mets shoor an  \n",
      "\n",
      "[33m 14s (1000 33.33333333333333%) 1.6880]\n",
      "A'\n",
      "\n",
      "'Bnainh for shal noo to have soind overy fakenid fir the merd,\n",
      "bling faiters pase, elf sarker be  \n",
      "\n",
      "[36m 48s (1100 36.666666666666664%) 1.6842]\n",
      "Aft hew dow young that\n",
      "of a for hot the having and half him troust, was with on thaugh, \n",
      "The becomes, \n",
      "\n",
      "[40m 21s (1200 40.0%) 1.7943]\n",
      "A DIside as me your men Enyin, Buszer and  Dode \n",
      "as dean't and when aSnables wenter spet wither Gows, \n",
      "\n",
      "[43m 51s (1300 43.333333333333336%) 1.8304]\n",
      "Aly \n",
      "anyther not of \n",
      "in and have and they but \n",
      "in, and a lonk to what with he said were and so list.  \n",
      "\n",
      "[47m 21s (1400 46.666666666666664%) 1.8342]\n",
      "Andle as conatiling conto in his face sides that the have interned a cart to the pad afturn!' said ch \n",
      "\n",
      "[50m 54s (1500 50.0%) 1.6973]\n",
      "As the \n",
      "had of Mr. jeggotty, the cande of it feer of \n",
      "could on had be it in a now you dand of a arms  \n",
      "\n",
      "[54m 26s (1600 53.333333333333336%) 1.8428]\n",
      "And that gineffully belise; the turnverfied\n",
      "in a parqually on upon him, and bear. There home of ant a \n",
      "\n",
      "[57m 58s (1700 56.666666666666664%) 1.6634]\n",
      "And and which as a whothouse home, and\n",
      "far at in along a not slowerand his eventled bothered to muste \n",
      "\n",
      "[61m 31s (1800 60.0%) 1.6952]\n",
      "And quite it as \n",
      "thesher of mind to can up appense to shough to make\n",
      "lack it, the clence.\n",
      "\n",
      "  \"To wood \n",
      "\n",
      "[65m 6s (1900 63.33333333333333%) 1.8814]\n",
      "At him our suce anfformidtamm had thes to canst his looking at valled\n",
      "of Defall to shair, shead!'  'I \n",
      "\n",
      "[68m 43s (2000 66.66666666666666%) 1.8307]\n",
      "Anntronnerdly old.  'As it to convees make of brand, that much inferted it that see him hardnain the  \n",
      "\n",
      "[72m 20s (2100 70.0%) 1.8983]\n",
      "A hast of the light he you.  Durdle after, I soming his extice.\n",
      "\n",
      "  \"Re are a sention you when the wou \n",
      "\n",
      "[75m 55s (2200 73.33333333333333%) 1.9133]\n",
      "Acked at she would wery nound bright he were to shanced and there have drearge old so candle with it  \n",
      "\n",
      "[79m 35s (2300 76.66666666666667%) 1.5210]\n",
      "A \n",
      "not\n",
      "after sulthing of then there respoding to how when then the hals onle know the my mother was m \n",
      "\n",
      "[83m 16s (2400 80.0%) 1.6893]\n",
      "ANCEARDoongring\n",
      "lict of my own me there among, sides to yought tool strill surning\n",
      "overy clead much d \n",
      "\n",
      "[87m 0s (2500 83.33333333333334%) 1.6366]\n",
      "AT\n",
      "Tholded his rages than yel of have to have had down can it is be and such if To more his legges la \n",
      "\n",
      "[90m 44s (2600 86.66666666666667%) 1.7736]\n",
      "Amose, and paintly should wall and a tire say excally say.  He book rish forth his instaken over lift \n",
      "\n",
      "[94m 27s (2700 90.0%) 1.4825]\n",
      "Am, when fone and follow.\n",
      "\n",
      "'You own fright with were one he repliean-Gist better me, Parged\n",
      "looket' w \n",
      "\n",
      "[98m 15s (2800 93.33333333333333%) 1.6885]\n",
      "AGE\n",
      "\n",
      "A Micasion that I was tokes disconfishis an agreaun with his  will obout out of with a chany sig \n",
      "\n",
      "[102m 0s (2900 96.66666666666667%) 1.4891]\n",
      "ANOPE NOTER to conded in the young (at the windowves and and he sometoo, and well, and wher\n",
      "om and a  \n",
      "\n",
      "[105m 47s (3000 100.0%) 1.7351]\n",
      "A to they trained than you said any little the was noled, you mother he shalls, the lettling so handn \n",
      "\n",
      "Minimal loss  1.5473023529052736\n",
      "BPC value  2.4709378237422692\n"
     ]
    }
   ],
   "source": [
    "change_hidden_size = [64, 128, 256, 512]\n",
    "result_of_chd_hidden_size = []\n",
    "\n",
    "for hidden_size in change_hidden_size:\n",
    "    print ('Learning model with hidden size {}, number of layers {}, learning rate {}'.format(hidden_size, 2, 0.005))\n",
    "    \n",
    "    trained_model, loss =parameters_tuning(hidden_size, n_layers=2, lr=0.005) \n",
    "    print ('Minimal loss ', min(loss))\n",
    "    \n",
    "    bits_per_character_result = bits_per_character(trained_model, test_tensor [:-1], test_tensor [1:])\n",
    "    print ('BPC value ', bits_per_character_result)\n",
    "    \n",
    "    result_of_chd_hidden_size.append('Hyperparameters: hidden size {}, number of layers {}, learning rate {}. \\n Evaluations: min loss {}, BPC {}'.format(hidden_size, 2, 0.005, min(loss), bits_per_character_result))\n",
    "#we need it later for out plotting exersize.\n",
    "result_of_chd_hidden_size.append('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnKuqy7ByBOZ",
    "outputId": "f2ce097f-af77-4605-e979-8fc345229e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model with hidden size 128, number of layers 1, learning rate 0.005\n",
      "[0m 14s (100 3.3333333333333335%) 2.1976]\n",
      "Aj, Mos thith wis herce hedderond ould nourke ther.es obe moplke maad thes he mabpally dou heredant i \n",
      "\n",
      "[0m 28s (200 6.666666666666667%) 2.1949]\n",
      "Aed for I ben whea\n",
      "capars, mp!'\n",
      "Phed ferins he shant, laly the an wentle she rouchet\n",
      "thais mast hat w \n",
      "\n",
      "[0m 43s (300 10.0%) 2.1117]\n",
      "Arvere lith an the\n",
      "thers I the all the the did live cornincound thind lound staby of the ner, inter,  \n",
      "\n",
      "[0m 58s (400 13.333333333333334%) 2.0455]\n",
      "An.   He ampall of Mr. ind lishing ald my ald to gothot she up, at trook he some knoing it. 'Wher.' f \n",
      "\n",
      "[1m 12s (500 16.666666666666664%) 2.1506]\n",
      "Ach and and to courbicing, wand, in and gofe\n",
      "to thours, in of his Mr. Grecatian by\n",
      "theanted was from  \n",
      "\n",
      "[1m 27s (600 20.0%) 1.8464]\n",
      "Aqunty in to I copper that were is nat\n",
      "low, bun'thed that hes and there has now tideft list had whid  \n",
      "\n",
      "[1m 41s (700 23.333333333333332%) 1.9584]\n",
      "Ago of a stret it with to sitted the woctles it my mits ondartied his the worded withat mase, and mos \n",
      "\n",
      "[1m 56s (800 26.666666666666668%) 1.9263]\n",
      "Axmetted hayt the somatitian.  I'\n",
      "\n",
      "Mr. \n",
      "Crewast have sidice time or one Roon Mr. \n",
      "'At it I chead I wa \n",
      "\n",
      "[2m 11s (900 30.0%) 2.0534]\n",
      "Ast in though ummone.  Lell be onays, and were.  But and he tome wame with pleation.  \"I hand which i \n",
      "\n",
      "[2m 25s (1000 33.33333333333333%) 1.8410]\n",
      "Ag a the look, and time dook, and my - dear-lear farmus fectent of adrway on have\n",
      "rom willeton the an \n",
      "\n",
      "[2m 40s (1100 36.666666666666664%) 1.9138]\n",
      "A2UI?T,\n",
      "Heis The loning again at selves what the the ladlight all was sighty haint were appark, very  \n",
      "\n",
      "[2m 55s (1200 40.0%) 1.7505]\n",
      "Anot amt as the preling was done that to exterenty to me facht.  'Coms of which for herd, and madeatt \n",
      "\n",
      "[3m 10s (1300 43.333333333333336%) 2.0142]\n",
      "Ad;\n",
      "'It'm' said hounder.  'Eate \n",
      "wand,\n",
      "insteln't Every mod, and his was morting exprestion.'\n",
      "\n",
      "'As \n",
      "Mr \n",
      "\n",
      "[3m 24s (1400 46.666666666666664%) 1.8452]\n",
      "A\u000b",
      "dmameth\n",
      "oned sone in the purtand of ithing and crope, and not one of her\n",
      "had becalmons, and of into \n",
      "\n",
      "[3m 39s (1500 50.0%) 1.7737]\n",
      "AELUENHER AA \n",
      "   \"I the remententy hussing to come to, and is shouf, and all ith the said and to come \n",
      "\n",
      "[3m 53s (1600 53.333333333333336%) 1.9843]\n",
      "AUPUSUSs poor to porminess from?'\n",
      "\n",
      "I cradial pratial \n",
      "the cauted the been forter to chang folent to t \n",
      "\n",
      "[4m 8s (1700 56.666666666666664%) 1.8841]\n",
      "ABLIT\n",
      "Lould with these not know a\n",
      "acquisted to\n",
      "be for we compark to a clonder his sone thelk as the c \n",
      "\n",
      "[4m 23s (1800 60.0%) 1.5797]\n",
      "Ah her it, hic.  He was comently, but neck, and reture as in the was was brot to exessindone of the d \n",
      "\n",
      "[4m 38s (1900 63.33333333333333%) 1.7061]\n",
      "AY this him, am I have more in thas it ham some a cour prether\n",
      "of with a sispeffring as out benome as \n",
      "\n",
      "[4m 52s (2000 66.66666666666666%) 1.5360]\n",
      "A)LYMr. Hoge in as she was cornistired that said the chasial, \"Shat\n",
      "firson a\n",
      "instih though last wordo \n",
      "\n",
      "[5m 7s (2100 70.0%) 1.8510]\n",
      "ANICPPA:NRTT ORAPSER.\n",
      "\n",
      "  'The clast my for\n",
      "chimbley she \n",
      "willemeftess with the arrishess be the cound \n",
      "\n",
      "[5m 22s (2200 73.33333333333333%) 1.7559]\n",
      "Ast Pundyes well - you the childly \n",
      "one stell in the good an upon the landerw, and but of the with th \n",
      "\n",
      "[5m 36s (2300 76.66666666666667%) 1.7803]\n",
      "Ad donaly Mr. Criss.  When I going and to and, and is a goned tith by wore and \n",
      "plaring air, touched, \n",
      "\n",
      "[5m 51s (2400 80.0%) 1.6603]\n",
      "AT.  I said Mrs. Mars bease as the\n",
      "drilled me have all the had before as \n",
      "all I bations at all come \n",
      " \n",
      "\n",
      "[6m 6s (2500 83.33333333333334%) 1.8536]\n",
      "ABs\u000b",
      "ed that I the consestiant of his nite of risisations, in the wall, \n",
      "and werds hards that the horl \n",
      "\n",
      "[6m 21s (2600 86.66666666666667%) 1.6809]\n",
      "ADE \n",
      "\n",
      "\n",
      "TI\n",
      "jeas that's some aways had pies that and the had Minain.  I light and down the oncerting th \n",
      "\n",
      "[6m 37s (2700 90.0%) 1.6109]\n",
      "A\n",
      "\n",
      "'do the was shold-to his puss of his a found for scimy dozliburhsed not were that look of she let  \n",
      "\n",
      "[6m 51s (2800 93.33333333333333%) 1.7733]\n",
      "ARND NnOOUSTEY Had stone of hers, was\n",
      "her the same ding.  He muck of a windows, as now, and as is \n",
      "'D \n",
      "\n",
      "[7m 6s (2900 96.66666666666667%) 1.7761]\n",
      "AI a all pass.  You had now at the get and was surch; from it petsenen in him mays my muther discome, \n",
      "\n",
      "[7m 20s (3000 100.0%) 1.6872]\n",
      "AWSOt at that out deased to and came, the had looked, or sider.  And mother the wrying out on far com \n",
      "\n",
      "Minimal loss  1.6311604919433595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per character value   2.515482391449389\n",
      "Learning model with hidden size 128, number of layers 2, learning rate 0.005\n",
      "[0m 23s (100 3.3333333333333335%) 2.4435]\n",
      "Awo tanches\n",
      "und cotild apsiche \n",
      "Tint des mit, on, I at od ind fu\n",
      "thn pontpered meaisggals ad sonmed m \n",
      "\n",
      "[0m 46s (200 6.666666666666667%) 2.2542]\n",
      "A:n\n",
      "\n",
      "derering ind wand dland winge pa\n",
      "be mererege hire wand and of hith\n",
      "soupon hapy in of of sangers  \n",
      "\n",
      "[1m 10s (300 10.0%) 2.0465]\n",
      "Ad sor my saked uplime in to sik, oth\n",
      "whhre youn the Anding on, ad, so lidises pretting hid wisted th \n",
      "\n",
      "[1m 33s (400 13.333333333333334%) 1.8875]\n",
      "A\n",
      "\n",
      "Pedster, and my staed he meculled breaon\n",
      "of thes sull neporure the the it anle so seem the a nosey \n",
      "\n",
      "[1m 57s (500 16.666666666666664%) 2.2854]\n",
      "Annopsing the culders wlond hand men to Dof muming by ind and bencen shere you dord my fally ind be m \n",
      "\n",
      "[2m 20s (600 20.0%) 1.9089]\n",
      "A there a distion thing as whis Mr.  You dom.  As the contor sowing ret, with this dist any in then i \n",
      "\n",
      "[2m 44s (700 23.333333333333332%) 2.0012]\n",
      "Ase have efre is agand it, wooking appangly lood in to hen the the streele ungist that Nethen to wome \n",
      "\n",
      "[3m 8s (800 26.666666666666668%) 1.9838]\n",
      "At, wut, and have whome jount a the asper of an thinch that\n",
      "his saightly and the retter that was was  \n",
      "\n",
      "[3m 31s (900 30.0%) 1.9405]\n",
      "A dirs, wha for kis puple been whis by and gow \n",
      "but and I susunty assed and \n",
      "me with hip so criss him \n",
      "\n",
      "[3m 55s (1000 33.33333333333333%) 2.2630]\n",
      "A me, and and Hut pair hem, tike it would taces of the was me lod of its gath\n",
      "morusion his his had th \n",
      "\n",
      "[4m 18s (1100 36.666666666666664%) 1.7607]\n",
      "A the kevererase no it.  You\n",
      "the peegased of tate.  Miss had the said my moker his or the spey there, \n",
      "\n",
      "[4m 41s (1200 40.0%) 1.7690]\n",
      "A\n",
      "\n",
      "  \"And her sees, had dound of\n",
      "fare crifi-y and-ret a did the had\n",
      "staer, and mone \n",
      "lind mentings my \n",
      "\n",
      "[5m 5s (1300 43.333333333333336%) 1.7736]\n",
      "A soless and there mame walged \n",
      "holdey with\n",
      "uppered, and the having the with a say her as its, and Mr \n",
      "\n",
      "[5m 28s (1400 46.666666666666664%) 1.7095]\n",
      "A\n",
      "- And eintlomither\n",
      "to that afwaysime, his ellad?\"\n",
      "\n",
      "  \"I I come have mecurt.  Rother Nest of that if \n",
      "\n",
      "[5m 52s (1500 50.0%) 1.9233]\n",
      "A \"A a heal her looken the very I have betty underable ard best of the he isway, and\n",
      "show camprauge h \n",
      "\n",
      "[6m 16s (1600 53.333333333333336%) 1.6439]\n",
      "A Wes beging unto an her the foll him as to the scedired a haddlow, and near-rust out of they eear ou \n",
      "\n",
      "[6m 39s (1700 56.666666666666664%) 1.9066]\n",
      "A some away the arposious, and Lour incow, and doot in he prearles in him.  'We known, and down were  \n",
      "\n",
      "[7m 3s (1800 60.0%) 1.6836]\n",
      "A He hearing on untered as Mr. Crait the his eressed about would were, and was hold well under alous  \n",
      "\n",
      "[7m 26s (1900 63.33333333333333%) 2.0099]\n",
      "Air, to winder of\n",
      "towally not roy gight.  \"By comester, thim the among the made of the but eit of ale \n",
      "\n",
      "[7m 50s (2000 66.66666666666666%) 1.8529]\n",
      "A exerver too anser.  'It ittire my know as his in and to my might and, soon for anaast.'\n",
      "\n",
      "'Do incond \n",
      "\n",
      "[8m 13s (2100 70.0%) 1.6064]\n",
      "A it mept a toutten not was nather that for somellown, and had distence, sin the seet; and be healloy \n",
      "\n",
      "[8m 37s (2200 73.33333333333333%) 1.6698]\n",
      "A\n",
      "\n",
      "\"CARE Ny?' Lhe supplack!' says all, they was in the contry by made you, where was suit-know man it \n",
      "\n",
      "[9m 1s (2300 76.66666666666667%) 1.9917]\n",
      "A obseed liftioness, as in him one, sountions of old supperseaty. \n",
      "It mure must and we were the boken \n",
      "\n",
      "[9m 25s (2400 80.0%) 1.5493]\n",
      "AFre. Affrenges inserd \n",
      "the ready be cooper frier of being \n",
      "he stair of the pait moge on the Dears so \n",
      "\n",
      "[9m 49s (2500 83.33333333333334%) 1.7456]\n",
      "AM Porsisation of him too Simery from a pity for be a who last git by, in a sort, when said that do i \n",
      "\n",
      "[10m 12s (2600 86.66666666666667%) 1.9929]\n",
      "Ad besing in men in the was were his heard.\n",
      "\n",
      "\n",
      "  \"I hand, \n",
      "all them the for the predeching at himpleti \n",
      "\n",
      "[10m 36s (2700 90.0%) 1.5444]\n",
      "Ary nown who man me, some be unseted good, as a his me had the cartion and ordsted to it to The sholl \n",
      "\n",
      "[11m 0s (2800 93.33333333333333%) 2.1343]\n",
      "As not to day compigould a face cause, soor very quite quite head with my minuse to him a happer.\n",
      "\n",
      "   \n",
      "\n",
      "[11m 24s (2900 96.66666666666667%) 1.5320]\n",
      "Amone make in the mother would her of but his outen as in his door of man to the vack to returned to  \n",
      "\n",
      "[11m 47s (3000 100.0%) 1.6631]\n",
      "A donder asted it, and I could at it, and see not man the like, it was one upon the fid the chile and \n",
      "\n",
      "Minimal loss  1.6119007568359376\n",
      "Bits per character value   2.4830412777972706\n",
      "Learning model with hidden size 128, number of layers 3, learning rate 0.005\n",
      "[0m 31s (100 3.3333333333333335%) 3.2112]\n",
      "A$YStw tdfi i it nn\n",
      "e  e      thahhh eae  eniaeoorwafii dt inS aie epianntdseudattaei\n",
      "qyatIe oeed\n",
      "r a \n",
      "\n",
      "[1m 2s (200 6.666666666666667%) 2.7863]\n",
      "A:TN3+`\f",
      "* tr we hos te te orf' ts,,e ho, npcnr saor naee, reghaele  hio csmahe nheeiw trs aaciee seee \n",
      "\n",
      "[1m 33s (300 10.0%) 2.8576]\n",
      "Akiae thr torr hh 'coas bnee belr manees sat. aSe uy wy gfuelaeelnn he, is aeawn! when, fid loetne an \n",
      "\n",
      "[2m 4s (400 13.333333333333334%) 2.4955]\n",
      "Atwith. \n",
      "\n",
      "  I whee to andse, waepsin, he ulde onder in oved whe phigelt. ake rere whid the sescos chh \n",
      "\n",
      "[2m 36s (500 16.666666666666664%) 2.4123]\n",
      "Al, hoiss, so loul surdest\n",
      "ad her inse a mistt mat ot kgacvisg lits as?\n",
      "  Peutegy chest \n",
      " \n",
      "sishethou, \n",
      "\n",
      "[3m 8s (600 20.0%) 2.0964]\n",
      "Ad corgaad, and readdy wangy\n",
      "a her whens, hit ibe, \n",
      "otidt the be on and, a and mur of on mound \n",
      "was n \n",
      "\n",
      "[3m 39s (700 23.333333333333332%) 2.2017]\n",
      "A but the the the'd thear hast litting he sterlss wistec\n",
      "the lattery a as desed yous the dare to Mres \n",
      "\n",
      "[4m 10s (800 26.666666666666668%) 2.0118]\n",
      "A \n",
      "'henthy did, the uftes.   Yough comes dependoss has \n",
      "fron sall of more in to ald mis on beliught o \n",
      "\n",
      "[4m 41s (900 30.0%) 1.8132]\n",
      "At he lousedsed the growgonte shaiter to looke,!  'Poster, potery,  the belred in chamenling a call h \n",
      "\n",
      "[5m 13s (1000 33.33333333333333%) 1.9749]\n",
      "As bece op into he comlinger in sare is been on hind pereln, hip my wamtione oor ve extifly firt and  \n",
      "\n",
      "[5m 44s (1100 36.666666666666664%) 2.1816]\n",
      "A\n",
      "\n",
      "'Fot for to the the po(thing it mith, Mrs dele it them it polly and a sat wime with yourd.  The le \n",
      "\n",
      "[6m 16s (1200 40.0%) 2.1241]\n",
      "A Lormer the\n",
      "was whuspled buthle ore in!  I have have to ouch opers, or norxed be them that was gath\n",
      " \n",
      "\n",
      "[6m 47s (1300 43.333333333333336%) 1.9756]\n",
      "At the care wead liptiin to seate the conation a be in the out the been up on beang as that no oum-th \n",
      "\n",
      "[7m 19s (1400 46.666666666666664%) 1.9461]\n",
      "Ak with leakle lom, and wallever a come all pleaes out of ther more his more hin wely sire I cevytour \n",
      "\n",
      "[7m 50s (1500 50.0%) 1.9739]\n",
      "A\n",
      "said ould twurned a frink.  Mund as in noten of my to saining and suld of shind diver the said her  \n",
      "\n",
      "[8m 22s (1600 53.333333333333336%) 2.1834]\n",
      "And bid dastingshir.  And to crase in whether have mone and was gray the mark,\n",
      "in anater for the he\n",
      "d \n",
      "\n",
      "[8m 54s (1700 56.666666666666664%) 1.9345]\n",
      "A Lortecty at the lost becain thing hor so and dearry conked said, who greel oneor the know that the  \n",
      "\n",
      "[9m 26s (1800 60.0%) 1.8557]\n",
      "As sooush and was was as hime fullice haby a susancardoring know.  I had agall to when they sert, sil \n",
      "\n",
      "[9m 58s (1900 63.33333333333333%) 2.0948]\n",
      "Ach to mutheranaised which in enceever, which 'and his dondow at expever and wheme cormar be that be  \n",
      "\n",
      "[10m 29s (2000 66.66666666666666%) 1.7365]\n",
      "Age.  We pous, and out to said up it undover, and wonce fached and mall a \n",
      "to I wath that the have ha \n",
      "\n",
      "[11m 1s (2100 70.0%) 1.8628]\n",
      "Ady sive is to which Miss we moman you epimpleton ear!  With Mr. Peggotty as that the wonke in you wa \n",
      "\n",
      "[11m 33s (2200 73.33333333333333%) 1.7629]\n",
      "At I look upon passassartication of pireffation lite of men of me my mostill putserigges look of his  \n",
      "\n",
      "[12m 4s (2300 76.66666666666667%) 1.8497]\n",
      "Atly saugh, he warday roy that shoulder, and me of you light a lardly air one.  I or the dine of my r \n",
      "\n",
      "[12m 36s (2400 80.0%) 1.8187]\n",
      "Ay sards which a dirss I was speames of a stroms to wass be winxing to light of a chinded that for,\n",
      "a \n",
      "\n",
      "[13m 7s (2500 83.33333333333334%) 1.6384]\n",
      "Aay mech for the comprees the\n",
      "it?' said Mr.  Cershire fator a mith are room, though as in any thround \n",
      "\n",
      "[13m 39s (2600 86.66666666666667%) 1.7139]\n",
      "Apremompred, I deplembir\n",
      "to mare berinced and our cury, the vant was lith to been the riddy \n",
      "couch fo \n",
      "\n",
      "[14m 10s (2700 90.0%) 2.1688]\n",
      "Ay year.\n",
      "\n",
      "'  \"\n",
      " \"I know you got had had side mate the stames, more of a did which you rarkis the she  \n",
      "\n",
      "[14m 43s (2800 93.33333333333333%) 1.6505]\n",
      "At the nance to me prestle\n",
      "whow Peggotty them.  And been of me been of he spirain anawired\n",
      "so me.\n",
      "MR. \n",
      "\n",
      "[15m 14s (2900 96.66666666666667%) 1.5850]\n",
      "A\n",
      "\n",
      "'No do it expan to little?\"\n",
      "\n",
      "  \"I how parter, and as Mr. Micoured in had shamence or my bought in  \n",
      "\n",
      "[15m 45s (3000 100.0%) 1.6193]\n",
      "Afarded me, \n",
      "eeme we setted had at\n",
      "the better-statidge of the houch not very was deaiction his dood b \n",
      "\n",
      "Minimal loss  1.6556673431396483\n",
      "Bits per character value   2.5469717723194316\n"
     ]
    }
   ],
   "source": [
    "layers_change = [1, 2, 3]\n",
    "\n",
    "for n_layers in layers_change:\n",
    "    print ('Learning model with hidden size {}, number of layers {}, learning rate {}'.format(128, n_layers, 0.005))\n",
    "    \n",
    "    trained_model, loss =parameters_tuning(hidden_size=128, n_layers=n_layers, lr=0.005) \n",
    "    print ('Minimal loss ', min(loss))\n",
    "    \n",
    "    bits_per_character_result2 = bits_per_character(trained_model, test_tensor [:-1], test_tensor [1:])\n",
    "    print ('Bits per character value  ', bits_per_character_result2)\n",
    "    \n",
    "    result_of_chd_hidden_size.append('Hyperparameters: hidden size {}, number of layers {}, learning rate {}. \\n Evaluations: min loss {}, BPC {}'.format(128, n_layers, 0.005, min(loss), bits_per_character_result2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2a0k-DJyBOb",
    "outputId": "4adb5291-7877-4959-a726-cf57754c85b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: hidden size 64, number of layers 2, learning rate 0.005. \n",
      " Evaluations: min loss 1.7532012786865234, BPC 2.68350434152888\n",
      "Hyperparameters: hidden size 128, number of layers 2, learning rate 0.005. \n",
      " Evaluations: min loss 1.5792749481201172, BPC 2.4553991078720423\n",
      "Hyperparameters: hidden size 256, number of layers 2, learning rate 0.005. \n",
      " Evaluations: min loss 1.5037402191162108, BPC 2.3807940837033374\n",
      "Hyperparameters: hidden size 512, number of layers 2, learning rate 0.005. \n",
      " Evaluations: min loss 1.5473023529052736, BPC 2.4709378237422692\n",
      "\n",
      "\n",
      "Hyperparameters: hidden size 128, number of layers 1, learning rate 0.005. \n",
      " Evaluations: min loss 1.6311604919433595, BPC 2.515482391449389\n",
      "Hyperparameters: hidden size 128, number of layers 2, learning rate 0.005. \n",
      " Evaluations: min loss 1.6119007568359376, BPC 2.4830412777972706\n",
      "Hyperparameters: hidden size 128, number of layers 3, learning rate 0.005. \n",
      " Evaluations: min loss 1.6556673431396483, BPC 2.5469717723194316\n"
     ]
    }
   ],
   "source": [
    "#Lets see what result we have \n",
    "for change in result_of_chd_hidden_size:\n",
    "    print(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hze0D0gHyBOc",
    "outputId": "81ba219a-8b6a-4aa4-928e-e0ec6a880e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning model with hidden size 128, number of layers 2, learning rate 0.001\n",
      "[0m 23s (100 3.3333333333333335%) 2.9935]\n",
      "AhYc tt i reamahg  t hn ,ynite e hee yhs t sa omr\n",
      "sdehr os ger rie luiai'n ibvtshsoa\n",
      "rt't\n",
      "yrat  ,tie  \n",
      "\n",
      "[0m 46s (200 6.666666666666667%) 2.5532]\n",
      "A^- eab lan ig nheret khalee oantted Mt enite Mtn to altatt whondl ihen int tres sye, nes innat blont \n",
      "\n",
      "[1m 9s (300 10.0%) 2.5042]\n",
      "As hay tan. bog mos is is hany tlroun waste\n",
      "se pamr he the ind wer,. ing we ae hhe anf errerithadurs  \n",
      "\n",
      "[1m 32s (400 13.333333333333334%) 2.2616]\n",
      "A; wuh ,ecatist pore whet coth ther bo edifet ans'mid meepe. wI wes. whe she sotind hevet rpeeca cs t \n",
      "\n",
      "[1m 55s (500 16.666666666666664%) 2.1603]\n",
      "A 'Ther thand I sued \n",
      "lere am anned tare, fhos or he the ferefpance prerith ans int whib, becousteder \n",
      "\n",
      "[2m 18s (600 20.0%) 2.2499]\n",
      "Alister, ser.\n",
      "\n",
      "\n",
      " \n",
      " gall;,' \"urke, and deapy any wyer ming and, I penas orseld leas The thas welr\n",
      "pree \n",
      "\n",
      "[2m 42s (700 23.333333333333332%) 2.2725]\n",
      "A uped fert ing \n",
      "her thas doat that as \n",
      "a the salrint feamy prwerpofed ot unthed untt or mer acer far \n",
      "\n",
      "[3m 5s (800 26.666666666666668%) 2.2067]\n",
      "Amjas ou that das thamire a the \n",
      "\n",
      "mamater\n",
      "agare memter the golict. bemed and the dother ard and the w \n",
      "\n",
      "[3m 28s (900 30.0%) 2.2620]\n",
      "A1icfertower the tieit, ereripole the twent at hage sore fore thas and wher so lelise o theven cacore \n",
      "\n",
      "[3m 51s (1000 33.33333333333333%) 1.9010]\n",
      "ALZ)xWTed don muts ind wave liace maun, Wher sand. \n",
      "'Yo unpe to bloth and hald foring the oper \n",
      "hefpe \n",
      "\n",
      "[4m 15s (1100 36.666666666666664%) 2.0775]\n",
      "A*ODNfure sair I sat to to lever sme dith hre dour pall yo a loonesen asparter wime pown ungor, s\n",
      "to  \n",
      "\n",
      "[4m 38s (1200 40.0%) 2.1503]\n",
      "A!g; and sisthal. \n",
      "\n",
      " \n",
      "\n",
      "Shable and So keancing atin at theor itting bever. Modering and \n",
      "it prodisseal \n",
      "\n",
      "[5m 2s (1300 43.333333333333336%) 1.9023]\n",
      "; as serlay wlo lout a heraghays ith wisted an the tloues, and be the prise and as wand taide the r \n",
      "\n",
      "[5m 25s (1400 46.666666666666664%) 1.8946]\n",
      "Azbunt and I watt and liss he me houch woulned wen's and inser don to the ment, and the were to that  \n",
      "\n",
      "[5m 49s (1500 50.0%) 2.0989]\n",
      "A\u000b",
      "whightner I mender and \"missoness to thit sait on oun-ender, the not at wam litter inserstaine, din \n",
      "\n",
      "[6m 12s (1600 53.333333333333336%) 1.9956]\n",
      "A7F \n",
      "facty allelight his the aly in rily the hit in the he gussey the saring of courmsay a srom an fe \n",
      "\n",
      "[6m 36s (1700 56.666666666666664%) 1.9449]\n",
      "A`&qkt shood uy the read caial prodef his wis of\n",
      "Mr. Carmentlred a to was a storen be on erintiond co \n",
      "\n",
      "[6m 59s (1800 60.0%) 1.9945]\n",
      "AOInd not colriggoth s stoners and heore in\n",
      "to disspared.  He rement of have all mistellly me after.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We experiment with the learning rate,hidden size is 128 and number of layers is 2.\n",
    "learning_change = [0.0001, 0.001, 0.01, 0.1]\n",
    "ler_change_learning = []\n",
    "\n",
    "for learning_rate in learning_change:\n",
    "    print ('Learning model with hidden size {}, number of layers {}, learning rate {}'.format(128, 2, learning_rate))\n",
    "    \n",
    "    trained_model, loss = parameters_tuning(hidden_size=128, n_layers=2, lr=learning_rate) \n",
    "    print ('Minimal loss ', min(loss))\n",
    "    \n",
    "    bits_per_character_result3 = bits_per_character(trained_model, test_tensor [:-1], test_tensor [1:])\n",
    "    print ('Bits per character value ', bits_per_character_result3)\n",
    "    \n",
    "    ler_change_learning .append('Hyperparameters: hidden size {}, number of layers {}, learning rate {}. \\n Evaluations: min loss {}, BPC {}'.format(128, 2, lr, min(loss), bits_per_character_result3))\n",
    "\n",
    "lerr_change_learning.append('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkbS-wjkyBOd",
    "outputId": "3489094b-016a-4d03-bc74-9bdae8c9835e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: hidden size 128, number of layers 2, learning rate 0.001. \n",
      " Evaluations: min loss 1.71063752746582, BPC 2.6064471687754445\n",
      "Hyperparameters: hidden size 128, number of layers 2, learning rate 0.001. \n",
      " Evaluations: min loss 1.6751178131103515, BPC 2.5483535096984653\n"
     ]
    }
   ],
   "source": [
    "#Lets see how out learning rate has changed\n",
    "for change_in_l in ler_change_learning:\n",
    "    print(change_in_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGQRsIkQiHTd"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qabs5XJgyBOd"
   },
   "source": [
    "# Plotting the Training Losses (20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cqBB44eyBOd"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "An important aspect of deep network training task is visualization. Visualizing the training loss values would be helpful for debugging the system. For instance, at extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. You would set the learning rate which do not cause oscillation with the help of visual charts.\n",
    "    \n",
    "In this exercise, we ask you to add the loss charts of experiments with different learning rates on the same graph and plot the graph. Add an entry for each experiment to the legend of the graph. If there is more than 10 experiments, use more than 1 chart (up to 10 experiments for each chart).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX9vuL4kyBOd",
    "outputId": "7d6098ee-fc68-434b-a1a8-ecf539a30499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig model with hidden size 128, number of layers 2, learning rate 0.0001\n",
      "[0m 18s (100 3.3333333333333335%) 3.4551]\n",
      "|L>`54H~xZ\n",
      "o)d;um a\n",
      " sfe zao as  sewtd sicng ntsagHnl rarosho a: b  i srm u& reetd   wctd+  \tg rae \n",
      "\n",
      "[0m 36s (200 6.666666666666667%) 3.3697]\n",
      "A>R]o0[H a.agted\n",
      ",nitna ut i  ret[he thls  teue ,eg_u  sstwthds fewne tht tnnm un ,oaosehs tnhoq rmos \n",
      "\n",
      "[0m 55s (300 10.0%) 2.9710]\n",
      "Amslg\u000b",
      "nf i\n",
      "asd rw  r\n",
      "etuthif a taihhytd eeitmyd osbnlth  Mhen edf?rn' woesbit\n",
      "8 ttue  eert tye  eaeo  \n",
      "\n",
      "[1m 14s (400 13.333333333333334%) 3.0887]\n",
      "A{nChmtye rtlhsaf \n",
      "n ,dgoNanmoetuifg e katlii  Podh'oei s eswhe\n",
      "la sewiasw nto wih roolil, abouimd i  \n",
      "\n",
      "[1m 32s (500 16.666666666666664%) 2.9727]\n",
      "AnQmo-rw eehevhI e ltrghalty bo oi rpstohi  late h\n",
      "wh oe  oaid u Ifeu yedd eao,e mse aias k\n",
      "aabaWo ef \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3740c42d29ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Trainig model with hidden size {}, number of layers {}, learning rate {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Minimal loss '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mchanges_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e28619ee677e>\u001b[0m in \u001b[0;36mparameters_tuning\u001b[0;34m(hidden_size, n_layers, lr)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#we just add some returning values,so we can call it later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ada592793dd3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(decoder, decoder_optimizer, inp, target)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e8f9c187c769>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden_lay)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_lay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMBD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mresult1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_zeros\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_lay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#here we will get output of the linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "changes_loss = {}\n",
    "learning_rate_plot = [0.001, 0.01, 0.1]\n",
    "\n",
    "for lr in learning_rate_plot:\n",
    "    print ('Trainig model with hidden size {}, number of layers {}, learning rate {}'.format(128, 2, lr))\n",
    "    \n",
    "    trained_model, loss = parameters_tuning(hidden_size=128, n_layers=2, lr=lr) \n",
    "    print ('Minimal loss ', min(loss))\n",
    "    changes_loss[lr] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6JfBPG7R0Ot"
   },
   "source": [
    "Unfortunataly in this notebook the cells with plots arent run already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq1T-_nck50F"
   },
   "source": [
    "# **Explanation:**\n",
    "\n",
    "I started a new notebook and then have lost the plot graph,however internet in Kazakhstan will be swtiched off soon and that's why you can not see the graph.\n",
    "X axis-the number of epochs(3000)\n",
    "\n",
    "Y axis-the loss values. \n",
    "Learning rate of 0.1 has the biggest steps in gradient decent and it this the reason why 0.1 is showing the worst results.The best resuts  belong to the learning rate 0.01.\n",
    "Also 0.001 is also showing not bad result,so lets chech the learning rates in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhdibzK7yBOd",
    "outputId": "0cd8b1b0-82eb-4614-bc57-edd009e1696f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAANSCAYAAADh7J46AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAf4ElEQVR4nO3dUcild33g8e9vJ5FprVtLzBY3E3FYUm0WzKLT6MWW2pXdJl5sKHQhsVRWCiGsKb00V+2FN9uLQhGjIUgQb5qLrbTpkip707pgw2YCNholMkTWvBvBSSwuKNk4+t+Led19+2aSOZk5Zyad+XzghXme5/+e87v5M8N3nuecWWsFAAAAwNXtn1zuAQAAAAC4/EQiAAAAAEQiAAAAAEQiAAAAABKJAAAAAEgkAgAAAKANItHMPDQz352Zr73C9ZmZT8zMqZl5cmbevf0xAQAAANilTe4k+mx126tcv726af/n7urTFz8WAAAAAJfSeSPRWutL1fdeZckd1efWWY9Vb56Zt25rQAAAAAB275otvMYN1bMHjvf2z33n8MKZubuzdxv1xje+8T3vfOc7t/D2AAAAAFQ98cQTz6+1rr+Q391GJJpznFvnWrjWerB6sOrEiRPr5MmTW3h7AAAAAKpm5n9e6O9u49vN9qobDxwfq57bwusCAAAAcIlsIxI9Un14/1vO3ld9f631skfNAAAAAHj9Ou/jZjPzp9X7q7fMzF71h9W1VWutB6pHqw9Wp6ofVh/Z1bAAAAAA7MZ5I9Fa667zXF/VR7c2EQAAAAD96Ec/am9vrxdffPFl144ePdqxY8e69tprt/Z+2/jgagAAAAC2bG9vrze96U29/e1vb+b/f2/YWqsXXnihvb29jh8/vrX328ZnEgEAAACwZS+++GLXXXfdPwhEVTPTddddd847jC6GSAQAAADwOnU4EJ3v/MUQiQAAAAAQiQAAAAAQiQAAAABet85+qfzm5y+GSAQAAADwOnT06NFeeOGFlwWhn3672dGjR7f6ftds9dUAAAAA2Ipjx461t7fX6dOnX3bt6NGjHTt2bKvvJxIBAAAAvA5de+21HT9+/JK9n8fNAAAAABCJAAAAABCJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgDaMRDNz28w8PTOnZua+c1z/+Zn5y5n5u5l5amY+sv1RAQAAANiV80aimTlS3V/dXt1c3TUzNx9a9tHq62utW6r3V388M2/Y8qwAAAAA7MgmdxLdWp1aaz2z1nqperi649CaVb1pZqb6uep71ZmtTgoAAADAzmwSiW6onj1wvLd/7qBPVr9cPVd9tfr9tdZPDr/QzNw9Mydn5uTp06cvcGQAAAAAtm2TSDTnOLcOHf9G9ZXqn1f/qvrkzPzTl/3SWg+utU6stU5cf/31r3FUAAAAAHZlk0i0V9144PhYZ+8YOugj1efXWaeqb1Xv3M6IAAAAAOzaJpHo8eqmmTm+/2HUd1aPHFrz7eoDVTPzi9U7qme2OSgAAAAAu3PN+Rastc7MzL3VF6sj1UNrradm5p796w9UH68+OzNf7ezjaR9baz2/w7kBAAAA2KLzRqKqtdaj1aOHzj1w4M/PVf9uu6MBAAAAcKls8rgZAAAAAFc4kQgAAAAAkQgAAAAAkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAGjDSDQzt83M0zNzambue4U175+Zr8zMUzPzN9sdEwAAAIBduuZ8C2bmSHV/9W+rverxmXlkrfX1A2veXH2qum2t9e2Z+Wc7mhcAAACAHdjkTqJbq1NrrWfWWi9VD1d3HFrzoerza61vV621vrvdMQEAAADYpU0i0Q3VsweO9/bPHfRL1S/MzF/PzBMz8+FtDQgAAADA7p33cbNqznFuneN13lN9oPqZ6m9n5rG11jf/wQvN3F3dXfW2t73ttU8LAAAAwE5scifRXnXjgeNj1XPnWPOFtdYP1lrPV1+qbjn8QmutB9daJ9ZaJ66//voLnRkAAACALdskEj1e3TQzx2fmDdWd1SOH1vxF9aszc83M/Gz13uob2x0VAAAAgF057+Nma60zM3Nv9cXqSPXQWuupmbln//oDa61vzMwXqiern1SfWWt9bZeDAwAAALA9s9bhjxe6NE6cOLFOnjx5Wd4bAAAA4Eo0M0+stU5cyO9u8rgZAAAAAFc4kQgAAAAAkQgAAAAAkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAGjDSDQzt83M0zNzambue5V1vzIzP56Z39reiAAAAADs2nkj0cwcqe6vbq9uru6amZtfYd0fVV/c9pAAAAAA7NYmdxLdWp1aaz2z1nqperi64xzrfq/6s+q7W5wPAAAAgEtgk0h0Q/XsgeO9/XP/z8zcUP1m9cD2RgMAAADgUtkkEs05zq1Dx39SfWyt9eNXfaGZu2fm5MycPH369IYjAgAAALBr12ywZq+68cDxseq5Q2tOVA/PTNVbqg/OzJm11p8fXLTWerB6sOrEiROHQxMAAAAAl8kmkejx6qaZOV79r+rO6kMHF6y1jv/0zzPz2eq/Hg5EAAAAALx+nTcSrbXOzMy9nf3WsiPVQ2utp2bmnv3rPocIAAAA4B+5Te4kaq31aPXooXPnjENrrf948WMBAAAAcClt8sHVAAAAAFzhRCIAAAAARCIAAAAARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIA2jEQzc9vMPD0zp2bmvnNc/+2ZeXL/58szc8v2RwUAAABgV84biWbmSHV/dXt1c3XXzNx8aNm3ql9ba72r+nj14LYHBQAAAGB3NrmT6Nbq1FrrmbXWS9XD1R0HF6y1vrzW+vv9w8eqY9sdEwAAAIBd2iQS3VA9e+B4b//cK/nd6q/OdWFm7p6ZkzNz8vTp05tPCQAAAMBObRKJ5hzn1jkXzvx6ZyPRx851fa314FrrxFrrxPXXX7/5lAAAAADs1DUbrNmrbjxwfKx67vCimXlX9Znq9rXWC9sZDwAAAIBLYZM7iR6vbpqZ4zPzhurO6pGDC2bmbdXnq99Za31z+2MCAAAAsEvnvZNorXVmZu6tvlgdqR5aaz01M/fsX3+g+oPquupTM1N1Zq11YndjAwAAALBNs9Y5P15o506cOLFOnjx5Wd4bAAAA4Eo0M09c6I07mzxuBgAAAMAVTiQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAADaMBLNzG0z8/TMnJqZ+85xfWbmE/vXn5yZd29/VAAAAAB25byRaGaOVPdXt1c3V3fNzM2Hlt1e3bT/c3f16S3PCQAAAMAObXIn0a3VqbXWM2utl6qHqzsOrbmj+tw667HqzTPz1i3PCgAAAMCOXLPBmhuqZw8c71Xv3WDNDdV3Di6ambs7e6dR1f+Zma+9pmmBbXhL9fzlHgKuUvYfXB72Hlwe9h5cHu+40F/cJBLNOc6tC1jTWuvB6sGqmTm51jqxwfsDW2TvweVj/8HlYe/B5WHvweUxMycv9Hc3edxsr7rxwPGx6rkLWAMAAADA69Qmkejx6qaZOT4zb6jurB45tOaR6sP733L2vur7a63vHH4hAAAAAF6fzvu42VrrzMzcW32xOlI9tNZ6ambu2b/+QPVo9cHqVPXD6iMbvPeDFzw1cDHsPbh87D+4POw9uDzsPbg8LnjvzVov++ggAAAAAK4ymzxuBgAAAMAVTiQCAAAAYPeRaGZum5mnZ+bUzNx3juszM5/Yv/7kzLx71zPB1WCDvffb+3vuyZn58szccjnmhCvN+fbegXW/MjM/npnfupTzwZVqk703M++fma/MzFMz8zeXeka4Um3w786fn5m/nJm/299/m3yGLfAqZuahmfnuzHztFa5fUGvZaSSamSPV/dXt1c3VXTNz86Flt1c37f/cXX16lzPB1WDDvfet6tfWWu+qPp4PFoSLtuHe++m6P+rsl0IAF2mTvTczb64+Vf37tda/rP7DpZ4TrkQb/t330erra61bqvdXf7z/zdnAhftsddurXL+g1rLrO4lurU6ttZ5Za71UPVzdcWjNHdXn1lmPVW+embfueC640p137621vrzW+vv9w8eqY5d4RrgSbfL3XtXvVX9WffdSDgdXsE323oeqz6+1vl211rL/YDs22X+retPMTPVz1feqM5d2TLiyrLW+1Nm99EouqLXsOhLdUD174Hhv/9xrXQO8Nq91X/1u9Vc7nQiuDufdezNzQ/Wb1QOXcC640m3y994vVb8wM389M0/MzIcv2XRwZdtk/32y+uXqueqr1e+vtX5yacaDq9YFtZZrdjbOWXOOc+sC1gCvzcb7amZ+vbOR6F/vdCK4Omyy9/6k+tha68dn/0MV2IJN9t411XuqD1Q/U/3tzDy21vrmroeDK9wm++83qq9U/6b6F9V/m5n/vtb63zueDa5mF9Radh2J9qobDxwf62w9fq1rgNdmo301M++qPlPdvtZ64RLNBleyTfbeierh/UD0luqDM3NmrfXnl2RCuDJt+m/O59daP6h+MDNfqm6pRCK4OJvsv49U/3mttapTM/Ot6p3V/7g0I8JV6YJay64fN3u8umlmju9/MNmd1SOH1jxSfXj/k7ffV31/rfWdHc8FV7rz7r2ZeVv1+ep3/C8qbM15995a6/ha6+1rrbdX/6X6TwIRXLRN/s35F9Wvzsw1M/Oz1Xurb1ziOeFKtMn++3Zn7+JrZn6xekf1zCWdEq4+F9Radnon0VrrzMzc29lvbzlSPbTWempm7tm//kD1aPXB6lT1w85WZuAibLj3/qC6rvrU/h0NZ9ZaJy7XzHAl2HDvAVu2yd5ba31jZr5QPVn9pPrMWuucXxsMbG7Dv/s+Xn12Zr7a2UdgPrbWev6yDQ1XgJn5085+W+BbZmav+sPq2rq41jJn7/gDAAAA4Gq268fNAAAAAPhHQCQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAKj+LwUw9oSLvRuhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we plot the changes in the learning rate\n",
    "fig, axis = plt.subplots(figsize = (20,15))\n",
    "epoch_line = np.arange(1, 3000, 10)#the number of epochs\n",
    "\n",
    "for element in changes_loss:\n",
    "    plt.plot(epoch_line, changes_loss[i], label=str(element))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZbKVCsOyBOe"
   },
   "outputs": [],
   "source": [
    "#Lets try our experiment once more with another learninng rate values\n",
    "changes_loss = {}\n",
    "lrate = [0.001, 0.003, 0.005, 0.007,0.009]\n",
    "\n",
    "for lr in learning_rate_plot:\n",
    "    print ('Trainig model with hidden size {}, number of layers {}, learning rate {}'.format(128, 2, lr))\n",
    "    \n",
    "    trained_model, loss = parameters_tuning(hidden_size=128, n_layers=2, lr=lr) \n",
    "    print ('Minimal loss ', min(loss))\n",
    "    changes_loss[lr] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T3V13TjlqsH"
   },
   "source": [
    "Here we can see that the change is not very big.So,we can keep 0,01 as a learning rate."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "abdullayeva-anlp-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
